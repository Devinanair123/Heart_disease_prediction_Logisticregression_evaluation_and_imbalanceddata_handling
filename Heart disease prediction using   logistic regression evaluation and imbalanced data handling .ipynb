{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93143a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              303 non-null    float64\n",
      " 1   gender           303 non-null    float64\n",
      " 2   cp               303 non-null    float64\n",
      " 3   trestbps         303 non-null    float64\n",
      " 4   chol             303 non-null    float64\n",
      " 5   fbs              303 non-null    float64\n",
      " 6   restecg          303 non-null    float64\n",
      " 7   thalach          303 non-null    float64\n",
      " 8   exang            303 non-null    float64\n",
      " 9   oldpeak          303 non-null    float64\n",
      " 10  slope            303 non-null    float64\n",
      " 11  ca               299 non-null    float64\n",
      " 12  thal             301 non-null    float64\n",
      " 13  heart_diagnosis  303 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 33.3 KB\n",
      "None\n",
      "              age      gender          cp    trestbps        chol         fbs  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
      "mean    54.438944    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
      "std      9.038662    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
      "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
      "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
      "50%     56.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
      "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
      "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
      "\n",
      "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  299.000000   \n",
      "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.672241   \n",
      "std      0.994971   22.875003    0.469794    1.161075    0.616226    0.937438   \n",
      "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000   \n",
      "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
      "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000   \n",
      "75%      2.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
      "max      2.000000  202.000000    1.000000    6.200000    3.000000    3.000000   \n",
      "\n",
      "             thal  heart_diagnosis  \n",
      "count  301.000000       303.000000  \n",
      "mean     4.734219         0.937294  \n",
      "std      1.939706         1.228536  \n",
      "min      3.000000         0.000000  \n",
      "25%      3.000000         0.000000  \n",
      "50%      3.000000         0.000000  \n",
      "75%      7.000000         2.000000  \n",
      "max      7.000000         4.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on Skewed Data:\n",
      "Accuracy: 0.6166666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        36\n",
      "           1       0.14      0.11      0.12         9\n",
      "           2       0.25      0.20      0.22         5\n",
      "           3       0.20      0.14      0.17         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62        60\n",
      "   macro avg       0.28      0.28      0.27        60\n",
      "weighted avg       0.54      0.62      0.57        60\n",
      "\n",
      "\n",
      "Results on Undersampled Data:\n",
      "Accuracy: 0.21666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.11      0.20        36\n",
      "           1       0.16      0.78      0.26         9\n",
      "           2       0.33      0.20      0.25         5\n",
      "           3       0.17      0.14      0.15         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.22        60\n",
      "   macro avg       0.29      0.25      0.17        60\n",
      "weighted avg       0.55      0.22      0.19        60\n",
      "\n",
      "\n",
      "Results on Oversampled Data:\n",
      "Accuracy: 0.5666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84        36\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.50      0.20      0.29         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.08      0.33      0.12         3\n",
      "\n",
      "    accuracy                           0.57        60\n",
      "   macro avg       0.28      0.28      0.25        60\n",
      "weighted avg       0.53      0.57      0.54        60\n",
      "\n",
      "\n",
      "Feature Importance from Skewed Data:\n",
      "[[ 0.00853943 -0.02141651 -0.00255054  0.06011924  0.03911322 -0.28167155\n",
      "  -0.57171047 -0.73747481  0.31789695 -0.32177446  0.05465179  0.07248938\n",
      "   0.24421568 -0.37523437  0.08674025  0.00606966 -0.09668743  0.15323304\n",
      "  -0.16274202  0.00563147  0.40220472  0.00370902 -0.40979125]\n",
      " [-0.02599529  0.00461416 -0.00272111 -0.10782794  0.01580027  0.08249451\n",
      "  -0.09300262  0.01210003 -0.13077362  0.13033281  0.02604119  0.00399598\n",
      "  -0.08603453  0.05555655  0.01129762 -0.01085095 -0.00088748 -0.02014974\n",
      "   0.05784817 -0.03813925 -0.07741759 -0.03657055  0.11354733]\n",
      " [-0.01404867 -0.00291267  0.00585217  0.00482425 -0.00588647  0.0907297\n",
      "   0.29052101  0.22441847 -0.07544384  0.07649967 -0.02215693 -0.0589741\n",
      "  -0.08156545  0.16375231  0.02654822  0.00682107 -0.03231347 -0.04813382\n",
      "   0.05109964 -0.00190999 -0.13096444  0.04035601  0.09166425]\n",
      " [-0.06056376  0.03742376  0.00176552  0.05926661 -0.02114935  0.12964603\n",
      "   0.22966794  0.31024987 -0.08901069  0.10209904 -0.04289935 -0.00152149\n",
      "  -0.03170127  0.08921046 -0.05082776 -0.01636239  0.08027849 -0.04949885\n",
      "   0.05069993  0.01188727 -0.14355807 -0.02969286  0.18633927]\n",
      " [ 0.09206829 -0.01770874 -0.00234604 -0.01638216 -0.02787767 -0.02119869\n",
      "   0.14452414  0.19070644 -0.02266879  0.01284294 -0.01563669 -0.01598977\n",
      "  -0.04491443  0.06671504 -0.07375833  0.0143226   0.04960988 -0.03545062\n",
      "   0.00309428  0.0225305  -0.05026462  0.02219837  0.0182404 ]]\n",
      "\n",
      "Feature Importance from Undersampled Data:\n",
      "[[ 0.01173549 -0.04412694 -0.0072542  -0.00206038  0.04479321 -0.01856851\n",
      "  -0.07381574 -0.09438076  0.05617693 -0.06135158  0.01584314 -0.01207493\n",
      "   0.05102337 -0.05996623  0.01541569  0.01211495 -0.0327053   0.03729103\n",
      "  -0.03387581 -0.00858988  0.04858158 -0.00564151 -0.04811472]\n",
      " [-0.0130791   0.0018067   0.00082915 -0.05271786  0.0119056  -0.01867122\n",
      "  -0.15979324 -0.15540456 -0.02060937  0.02384747  0.01893244  0.0324584\n",
      "  -0.00374279 -0.04440995  0.02815625 -0.00897356 -0.01594459  0.00271006\n",
      "   0.01904819 -0.01852015  0.04978718 -0.02024879 -0.02630028]\n",
      " [-0.01553453  0.00640438  0.00342517  0.01332007 -0.00338874  0.01505113\n",
      "   0.10078149  0.03823152 -0.00584376  0.00622801 -0.00787386 -0.02355852\n",
      "  -0.02511313  0.05692976  0.02426054  0.0011668  -0.02504309 -0.01267682\n",
      "   0.01219384  0.00086722 -0.03262659  0.02795251  0.00505833]\n",
      " [-0.02299574  0.02554062  0.00167162  0.04733822 -0.0178126   0.04969827\n",
      "   0.0712254   0.11899574 -0.02742798  0.03576999 -0.02034992  0.00946028\n",
      "  -0.00020039  0.01943203 -0.02282654 -0.01256894  0.04373749 -0.01253765\n",
      "   0.01040737  0.01047228 -0.04880217 -0.0177784   0.07492257]\n",
      " [ 0.03987388  0.01037524  0.00132826 -0.00588005 -0.03549748 -0.02750968\n",
      "   0.06160209  0.09255807 -0.00229581 -0.00449388 -0.0065518  -0.00628523\n",
      "  -0.02196705  0.02801438 -0.04500594  0.00826075  0.0299555  -0.01478663\n",
      "  -0.00777359  0.01577053 -0.01694     0.01571619 -0.00556589]]\n",
      "\n",
      "Feature Importance from Oversampled Data:\n",
      "[[ 2.77719298e-02 -2.41539825e-02 -6.73374462e-03  5.23419053e-02\n",
      "   3.74048494e-02 -1.50100493e-01 -3.84397350e-01 -5.32515517e-01\n",
      "   1.93746420e-01 -1.89650521e-01  4.11122355e-02  4.91626692e-02\n",
      "   1.55404819e-01 -2.41583826e-01  9.91982471e-02 -1.46507450e-02\n",
      "  -8.04516036e-02  8.37520106e-02 -6.43708821e-02 -1.52852300e-02\n",
      "   2.05034371e-01 -2.13096432e-02 -1.79628829e-01]\n",
      " [-7.80811893e-02  9.25085070e-04  4.76006269e-03 -4.03390873e-02\n",
      "   2.28161984e-02  4.59563983e-02 -1.19433687e-01 -1.03660325e-01\n",
      "  -4.73323710e-02  5.08371295e-02  2.17682934e-02  1.02083767e-02\n",
      "  -1.63247629e-02 -1.21471487e-02  4.64337579e-02 -1.43398159e-02\n",
      "  -2.85891836e-02  5.40873505e-03  2.40828949e-02 -2.59868714e-02\n",
      "  -2.37466592e-02 -3.01441900e-02  5.73956077e-02]\n",
      " [ 8.67743248e-02 -1.02715391e-02  5.01753558e-03  8.38575893e-03\n",
      "  -3.97585173e-02  6.00280065e-02  6.57032082e-02 -1.81088592e-02\n",
      "  -2.92184603e-02  3.58824056e-02 -2.96628444e-03 -1.85292669e-02\n",
      "  -1.31217265e-02  4.12812231e-02  5.77173713e-02 -5.55482881e-03\n",
      "  -4.54985972e-02 -7.10649712e-03  2.20275018e-02 -8.25705938e-03\n",
      "  -3.89320022e-02  6.02141573e-03  3.95745318e-02]\n",
      " [-6.33557409e-02  2.56668861e-02  3.25152919e-03  3.01987865e-02\n",
      "  -9.20155376e-03  8.67796179e-02  5.70482771e-02  3.01202185e-02\n",
      "  -3.91633719e-02  4.89751368e-02 -1.15187928e-02  6.44696492e-03\n",
      "   7.23637425e-03  7.64721856e-03  1.99995915e-02 -9.87331447e-03\n",
      "  -3.14512063e-04 -2.40995064e-03  6.25959652e-03  5.96211904e-03\n",
      "  -4.26966821e-02 -1.83932741e-02  7.09017212e-02]\n",
      " [ 2.68906755e-02  7.83355050e-03 -6.29538285e-03 -5.05873635e-02\n",
      "  -1.12609767e-02 -4.26635302e-02  3.81079552e-01  6.24164483e-01\n",
      "  -7.80322168e-02  5.39558496e-02 -4.83954517e-02 -4.72887440e-02\n",
      "  -1.33194704e-01  2.04802533e-01 -2.23348968e-01  4.44187041e-02\n",
      "   1.54853896e-01 -7.96442978e-02  1.20008889e-02  4.35670417e-02\n",
      "  -9.96590269e-02  6.38256915e-02  1.17569682e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RFE Feature Ranking:\n",
      "age: 20\n",
      "trestbps: 22\n",
      "chol: 23\n",
      "fbs: 7\n",
      "thalach: 21\n",
      "exang: 8\n",
      "oldpeak: 15\n",
      "ca: 6\n",
      "gender_0.0: 10\n",
      "gender_1.0: 17\n",
      "cp_1.0: 14\n",
      "cp_2.0: 11\n",
      "cp_3.0: 9\n",
      "cp_4.0: 1\n",
      "restecg_0.0: 2\n",
      "restecg_1.0: 19\n",
      "restecg_2.0: 12\n",
      "slope_1.0: 5\n",
      "slope_2.0: 18\n",
      "slope_3.0: 4\n",
      "thal_3.0: 13\n",
      "thal_6.0: 16\n",
      "thal_7.0: 3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Step 1: Read the heart dataset\n",
    "heart_df = pd.read_csv('heart_disease (1).csv')  # Adjust the filename as per your dataset\n",
    "\n",
    "# Step 2: Exploratory Data Analysis (EDA)\n",
    "# Data Quality Check\n",
    "print(heart_df.info())\n",
    "print(heart_df.describe())\n",
    "\n",
    "# Treat Missing Values if any\n",
    "heart_df.dropna(inplace=True)  # Example: Drop rows with missing values, you might want a more sophisticated approach\n",
    "\n",
    "# Step 3: Transform Categorical Data\n",
    "# Example: Convert categorical columns to numerical using one-hot encoding\n",
    "heart_df = pd.get_dummies(heart_df, columns=['gender', 'cp', 'restecg', 'slope', 'thal'])\n",
    "\n",
    "# Step 4: Split the data into the train and test set\n",
    "X = heart_df.drop('heart_diagnosis', axis=1)\n",
    "y = heart_df['heart_diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Undersample and oversample the data\n",
    "# Undersample using RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_undersampled, y_undersampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Oversample using RandomOverSampler\n",
    "oversampler = RandomOverSampler(sampling_strategy='minority')\n",
    "X_oversampled, y_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 6: Apply the Logistic Regression model on the skewed data, undersampled data, and oversampled data\n",
    "# Logistic Regression on Skewed Data\n",
    "model_skewed = LogisticRegression(random_state=42)\n",
    "model_skewed.fit(X_train, y_train)\n",
    "\n",
    "# Logistic Regression on Undersampled Data\n",
    "model_undersampled = LogisticRegression(random_state=42)\n",
    "model_undersampled.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "# Logistic Regression on Oversampled Data\n",
    "model_oversampled = LogisticRegression(random_state=42)\n",
    "model_oversampled.fit(X_oversampled, y_oversampled)\n",
    "\n",
    "# Step 7: Print the model results\n",
    "# Skewed Data\n",
    "print(\"\\nResults on Skewed Data:\")\n",
    "y_pred_skewed = model_skewed.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_skewed))\n",
    "print(classification_report(y_test, y_pred_skewed))\n",
    "\n",
    "# Undersampled Data\n",
    "print(\"\\nResults on Undersampled Data:\")\n",
    "y_pred_undersampled = model_undersampled.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_undersampled))\n",
    "print(classification_report(y_test, y_pred_undersampled))\n",
    "\n",
    "# Oversampled Data\n",
    "print(\"\\nResults on Oversampled Data:\")\n",
    "y_pred_oversampled = model_oversampled.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_oversampled))\n",
    "print(classification_report(y_test, y_pred_oversampled))\n",
    "\n",
    "# Step 8: Get the feature importance\n",
    "# Feature importance from the Skewed Data Model\n",
    "print(\"\\nFeature Importance from Skewed Data:\")\n",
    "print(model_skewed.coef_)\n",
    "\n",
    "# Feature importance from the Undersampled Data Model\n",
    "print(\"\\nFeature Importance from Undersampled Data:\")\n",
    "print(model_undersampled.coef_)\n",
    "\n",
    "# Feature importance from the Oversampled Data Model\n",
    "print(\"\\nFeature Importance from Oversampled Data:\")\n",
    "print(model_oversampled.coef_)\n",
    "\n",
    "# Additional: You can use Recursive Feature Elimination (RFE) to get more refined feature importance\n",
    "selector = RFE(model_oversampled, n_features_to_select=1)\n",
    "selector.fit(X_oversampled, y_oversampled)\n",
    "\n",
    "print(\"\\nRFE Feature Ranking:\")\n",
    "for feature, rank in zip(X.columns, selector.ranking_):\n",
    "    print(f'{feature}: {rank}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adc159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
